{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary.torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from utils.utils import load_specific_image, read_image, read_mask, add_mask\n",
    "from CONSTS import IMAGEPATH, MASKPATH\n",
    "\n",
    "from networks.discriminator import PatchDiscriminator\n",
    "from networks.generator import ResNetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fashion_swapper_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, loader, obj=31, transform=None):\n",
    "        self.objects = obj\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        first_name = self.loader['objects'][self.objects][idx]\n",
    "        first_image = read_image(first_name)\n",
    "        \n",
    "        first_mask = read_mask(first_name, self.objects)\n",
    "        first_mask = Image.fromarray(first_mask)\n",
    "        first_image = self.transform(first_image)\n",
    "        first_mask = self.transform(first_mask)\n",
    "        imagewithmask = add_mask(first_image, first_mask, 0)\n",
    "        return imagewithmask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.loader['objects_count'][self.objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createswapper_loader(image_path, mask_path, object_one=31, object_two=40):\n",
    "    trans = transforms.Compose([transforms.Resize((300, 200), 2), transforms.ToTensor()])\n",
    "    first_object = load_specific_image(IMAGEPATH, MASKPATH, objects=[object_one, object_two])\n",
    "    \n",
    "    dataset_one = Fashion_swapper_dataset(first_object, object_one, transform=trans)\n",
    "    dataset_second = Fashion_swapper_dataset(first_object, object_two, transform=trans)\n",
    "    loader_one = DataLoader(dataset_one, batch_size=8, shuffle=True, drop_last=True)\n",
    "    loader_second = DataLoader(dataset_second, batch_size=8, shuffle=True, drop_last=True)\n",
    "    return loader_one, loader_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(c_dim=4, g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n",
    "    G_XtoY = ResNetGenerator(conv_dim=g_conv_dim, c_dim=c_dim, repeat_num=n_res_blocks)\n",
    "    G_YtoX = ResNetGenerator(conv_dim=g_conv_dim, c_dim=c_dim, repeat_num=n_res_blocks)\n",
    "    \n",
    "    D_X = PatchDiscriminator(c_dim=c_dim, conv_dim=d_conv_dim)\n",
    "    D_Y = PatchDiscriminator(c_dim=c_dim, conv_dim=d_conv_dim)\n",
    "\n",
    "    # move models to GPU, if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        G_XtoY.to(device)\n",
    "        G_YtoX.to(device)\n",
    "        D_X.to(device)\n",
    "        D_Y.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1004 [00:00<00:21, 46.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004/1004 [00:16<00:00, 60.54it/s]\n"
     ]
    }
   ],
   "source": [
    "G_XtoY, G_YtoX, D_X, D_Y = create_model()\n",
    "loader_pants, loader_shirts = createswapper_loader(IMAGEPATH, MASKPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 100]           4,160\n",
      "         LeakyReLU-2         [-1, 64, 150, 100]               0\n",
      "      SpectralNorm-3          [-1, 128, 75, 50]               0\n",
      "       BatchNorm2d-4          [-1, 128, 75, 50]             256\n",
      "         LeakyReLU-5          [-1, 128, 75, 50]               0\n",
      "      SpectralNorm-6          [-1, 256, 37, 25]               0\n",
      "       BatchNorm2d-7          [-1, 256, 37, 25]             512\n",
      "         LeakyReLU-8          [-1, 256, 37, 25]               0\n",
      "      SpectralNorm-9          [-1, 512, 36, 24]               0\n",
      "      BatchNorm2d-10          [-1, 512, 36, 24]           1,024\n",
      "        LeakyReLU-11          [-1, 512, 36, 24]               0\n",
      "           Conv2d-12            [-1, 1, 36, 24]           4,609\n",
      "================================================================\n",
      "Total params: 10,561\n",
      "Trainable params: 10,561\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.92\n",
      "Forward/backward pass size (MB): 41.19\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 42.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(D_X, (4, 300, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_masks(self, segs):\n",
    "    \"\"\"Merge masks (B, N, W, H) -> (B, 1, W, H)\"\"\"\n",
    "    ret = torch.sum((segs + 1)/2, dim=1, keepdim=True)  # (B, 1, W, H)\n",
    "    return ret.clamp(max=1, min=0) * 2 - 1\n",
    "\n",
    "def get_weight_for_ctx(self, x, y):\n",
    "    \"\"\"Get weight for context preserving loss\"\"\"\n",
    "    z = self.merge_masks(torch.cat([x, y], dim=1))\n",
    "    return (1 - z) / 2\n",
    "\n",
    "def weighted_L1_loss(self, src, tgt, weight):\n",
    "    \"\"\"L1 loss with given weight (used for context preserving loss)\"\"\"\n",
    "    return weight * torch.mean(torch.abs(src - tgt))\n",
    "    \n",
    "def real_mse_loss(D_out):\n",
    "    return torch.mean((D_out-1)**2)\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    return torch.mean(D_out**2)\n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    reconstr_loss = torch.nn.L1Loss()\n",
    "    return lambda_weight*reconstr_loss(real_im, reconstructed_im)\n",
    "\n",
    "# def l2_loss(real_im, reconstructed_im, lambda_weight):\n",
    "#     reconstr_loss = torch.nn.MSELoss()\n",
    "#     return reconstr_loss(real_im, reconstructed_im)*lambda_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004/1004 [00:23<00:00, 43.43it/s]\n"
     ]
    }
   ],
   "source": [
    "loader_pants, loader_shorts = createswapper_loader(IMAGEPATH, MASKPATH, object_one=31, object_two=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "lr_g=0.0002\n",
    "lr_d=0.0001\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "\n",
    "g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
    "\n",
    "# Create optimizers for the generators and discriminators\n",
    "optimizer_G = torch.optim.Adam(filter(lambda p: p.requires_grad, itertools.chain(G_XtoY.parameters(), \n",
    "                                                                                 G_YtoX.parameters())),\n",
    "                               lr=lr_g, \n",
    "                               betas=(beta1, beta2))\n",
    "optimizer_D = torch.optim.Adam(filter(lambda p: p.requires_grad, itertools.chain(D_X.parameters(), \n",
    "                                                                                 D_Y.parameters())), \n",
    "                               lr=lr_d, \n",
    "                               betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-eb392e6764f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_pants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a = iter(loader_pants)\n",
    "images_X, _ = a.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n",
    "                  n_epochs=1000):\n",
    "    \n",
    "    print_every=10\n",
    "    \n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    \n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = test_iter_X.next()[0]\n",
    "    fixed_Y = test_iter_Y.next()[0]\n",
    "    fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1\n",
    "    fixed_Y = scale(fixed_Y)\n",
    "\n",
    "    # batches per epoch\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "    batches_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "\n",
    "        # Reset iterators for each epoch\n",
    "        if epoch % batches_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, _ = iter_X.next()\n",
    "        images_X = scale(images_X) # make sure to scale to a range -1 to 1\n",
    "\n",
    "        images_Y, _ = iter_Y.next()\n",
    "        images_Y = scale(images_Y)\n",
    "        \n",
    "        # move images to GPU if available (otherwise stay on CPU)\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images_X = images_X.to(device)\n",
    "        images_Y = images_Y.to(device)\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        ##   First: D_X, real and fake loss components   ##\n",
    "\n",
    "        # Train with real images\n",
    "        d_x_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Compute the discriminator losses on real images\n",
    "        out_x = D_X(images_X)\n",
    "        D_X_real_loss = real_mse_loss(out_x)\n",
    "        \n",
    "        # Train with fake images\n",
    "        \n",
    "        # 2. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = G_YtoX(images_Y)\n",
    "\n",
    "        # 3. Compute the fake loss for D_X\n",
    "        out_x = D_X(fake_X)\n",
    "        D_X_fake_loss = fake_mse_loss(out_x)\n",
    "        \n",
    "\n",
    "        # 4. Compute the total loss and perform backprop\n",
    "        d_x_loss = D_X_real_loss + D_X_fake_loss\n",
    "        d_x_loss.backward()\n",
    "        d_x_optimizer.step()\n",
    "\n",
    "        \n",
    "        ##   Second: D_Y, real and fake loss components   ##\n",
    "        \n",
    "        # Train with real images\n",
    "        d_y_optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Compute the discriminator losses on real images\n",
    "        out_y = D_Y(images_Y)\n",
    "        D_Y_real_loss = real_mse_loss(out_y)\n",
    "        \n",
    "        # Train with fake images\n",
    "\n",
    "        # 2. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = G_XtoY(images_X)\n",
    "\n",
    "        # 3. Compute the fake loss for D_Y\n",
    "        out_y = D_Y(fake_Y)\n",
    "        D_Y_fake_loss = fake_mse_loss(out_y)\n",
    "\n",
    "        # 4. Compute the total loss and perform backprop\n",
    "        d_y_loss = D_Y_real_loss + D_Y_fake_loss\n",
    "        d_y_loss.backward()\n",
    "        d_y_optimizer.step()\n",
    "\n",
    "\n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "        ##    First: generate fake X images and reconstructed Y images    ##\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        # 1. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = G_YtoX(images_Y)\n",
    "\n",
    "        # 2. Compute the generator loss based on domain X\n",
    "        out_x = D_X(fake_X)\n",
    "        g_YtoX_loss = real_mse_loss(out_x)\n",
    "\n",
    "        # 3. Create a reconstructed y\n",
    "        # 4. Compute the cycle consistency loss (the reconstruction loss)\n",
    "        reconstructed_Y = G_XtoY(fake_X)\n",
    "        reconstructed_y_loss = cycle_consistency_loss(images_Y, reconstructed_Y, lambda_weight=10)\n",
    "\n",
    "\n",
    "        ##    Second: generate fake Y images and reconstructed X images    ##\n",
    "\n",
    "        # 1. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = G_XtoY(images_X)\n",
    "\n",
    "        # 2. Compute the generator loss based on domain Y\n",
    "        out_y = D_Y(fake_Y)\n",
    "        g_XtoY_loss = real_mse_loss(out_y)\n",
    "\n",
    "        # 3. Create a reconstructed x\n",
    "        # 4. Compute the cycle consistency loss (the reconstruction loss)\n",
    "        reconstructed_X = G_YtoX(fake_Y)\n",
    "        reconstructed_x_loss = cycle_consistency_loss(images_X, reconstructed_X, lambda_weight=10)\n",
    "\n",
    "        # 5. Add up all generator and reconstructed losses and perform backprop\n",
    "        g_total_loss = g_YtoX_loss + g_XtoY_loss + reconstructed_y_loss + reconstructed_x_loss\n",
    "        g_total_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print the log info\n",
    "        if epoch % print_every == 0:\n",
    "            # append real and fake discriminator losses and the generator loss\n",
    "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\n",
    "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "\n",
    "            \n",
    "        sample_every=100\n",
    "        # Save the generated samples\n",
    "        if epoch % sample_every == 0:\n",
    "            G_YtoX.eval() # set generators to eval mode for sample generation\n",
    "            G_XtoY.eval()\n",
    "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16)\n",
    "            G_YtoX.train()\n",
    "            G_XtoY.train()\n",
    "\n",
    "        # uncomment these lines, if you want to save your model\n",
    "#         checkpoint_every=1000\n",
    "#         # Save the model parameters\n",
    "#         if epoch % checkpoint_every == 0:\n",
    "#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
