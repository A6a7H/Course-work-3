{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary.torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from utils.utils import load_specific_image, read_image, read_mask, add_mask\n",
    "from CONSTS import IMAGEPATH, MASKPATH\n",
    "\n",
    "from networks.discriminator import PatchDiscriminator\n",
    "from networks.generator import ResNetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fashion_swapper_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, loader, objects=[31, 40], transform=None):\n",
    "        self.objects = objects\n",
    "        self.transform = transform\n",
    "        self.loader = loader\n",
    "        self.len_first_obj = loader['objects_count'][self.objects[0]]\n",
    "        self.len_second_obj = loader['objects_count'][self.objects[1]]\n",
    "        \n",
    "    def __getitem__(self, idx):        \n",
    "        first_name = self.loader['objects'][self.objects[0]][idx % self.len_first_obj]\n",
    "        first_image = read_image(first_name)\n",
    "        \n",
    "        first_mask = read_mask(first_name, self.objects[0])\n",
    "        first_mask = Image.fromarray(first_mask)\n",
    "        first_image = self.transform(first_image)\n",
    "        first_mask = self.transform(first_mask)\n",
    "        imagewithmask_first = add_mask(first_image, first_mask, axis=0)\n",
    "        \n",
    "        second_name = self.loader['objects'][self.objects[1]][idx % self.len_second_obj]\n",
    "        second_image = read_image(second_name)\n",
    "        \n",
    "        second_mask = read_mask(first_name, self.objects[1])\n",
    "        second_mask = Image.fromarray(second_mask)\n",
    "        second_image = self.transform(second_image)\n",
    "        second_mask = self.transform(second_mask)\n",
    "        imagewithmask_second = add_mask(second_image, second_mask, axis=0)\n",
    "        \n",
    "        return imagewithmask_first, imagewithmask_second\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.len_first_obj, self.len_second_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createswapper_loader(image_path, mask_path, object_one=31, object_two=40):\n",
    "    trans = transforms.Compose([transforms.Resize((300, 200), 2), transforms.ToTensor()])\n",
    "    first_object = load_specific_image(IMAGEPATH, MASKPATH, objects=[object_one, object_two])\n",
    "    \n",
    "    dataset_one = Fashion_swapper_dataset(first_object, objects=[object_one, object_two], transform=trans)\n",
    "    loader_one = DataLoader(dataset_one, batch_size=4, shuffle=True)\n",
    "    return loader_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(c_dim=4, g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n",
    "    G_XtoY = ResNetGenerator(conv_dim=g_conv_dim, c_dim=c_dim, repeat_num=n_res_blocks)\n",
    "    G_YtoX = ResNetGenerator(conv_dim=g_conv_dim, c_dim=c_dim, repeat_num=n_res_blocks)\n",
    "    \n",
    "    D_X = PatchDiscriminator(c_dim=c_dim, conv_dim=d_conv_dim)\n",
    "    D_Y = PatchDiscriminator(c_dim=c_dim, conv_dim=d_conv_dim)\n",
    "\n",
    "    # move models to GPU, if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        G_XtoY.to(device)\n",
    "        G_YtoX.to(device)\n",
    "        D_X.to(device)\n",
    "        D_Y.to(device)\n",
    "        print('Models moved to GPU.')\n",
    "    else:\n",
    "        print('Only CPU available.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1004 [00:00<00:16, 59.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only CPU available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1004/1004 [00:14<00:00, 70.20it/s]\n"
     ]
    }
   ],
   "source": [
    "G_AtoB, G_BtoA, D_A, D_B = create_model()\n",
    "loader_pants = createswapper_loader(IMAGEPATH, MASKPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 150, 100]           4,160\n",
      "         LeakyReLU-2         [-1, 64, 150, 100]               0\n",
      "      SpectralNorm-3          [-1, 128, 75, 50]               0\n",
      "       BatchNorm2d-4          [-1, 128, 75, 50]             256\n",
      "         LeakyReLU-5          [-1, 128, 75, 50]               0\n",
      "      SpectralNorm-6          [-1, 256, 37, 25]               0\n",
      "       BatchNorm2d-7          [-1, 256, 37, 25]             512\n",
      "         LeakyReLU-8          [-1, 256, 37, 25]               0\n",
      "      SpectralNorm-9          [-1, 512, 36, 24]               0\n",
      "      BatchNorm2d-10          [-1, 512, 36, 24]           1,024\n",
      "        LeakyReLU-11          [-1, 512, 36, 24]               0\n",
      "           Conv2d-12            [-1, 1, 36, 24]           4,609\n",
      "================================================================\n",
      "Total params: 10,561\n",
      "Trainable params: 10,561\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.92\n",
      "Forward/backward pass size (MB): 41.19\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 42.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(D_A, (4, 300, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_masks(self, segs):\n",
    "    \"\"\"Merge masks (B, N, W, H) -> (B, 1, W, H)\"\"\"\n",
    "    ret = torch.sum((segs + 1)/2, dim=1, keepdim=True)  # (B, 1, W, H)\n",
    "    return ret.clamp(max=1, min=0) * 2 - 1\n",
    "\n",
    "def get_weight_for_ctx(self, x, y):\n",
    "    \"\"\"Get weight for context preserving loss\"\"\"\n",
    "    z = self.merge_masks(torch.cat([x, y], dim=1))\n",
    "    return (1 - z) / 2\n",
    "\n",
    "def weighted_L1_loss(self, src, tgt, weight):\n",
    "    \"\"\"L1 loss with given weight (used for context preserving loss)\"\"\"\n",
    "    return weight * torch.mean(torch.abs(src - tgt))\n",
    "    \n",
    "def real_mse_loss(D_out):\n",
    "    return torch.mean((D_out-1)**2)\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    return torch.mean(D_out**2)\n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    reconstr_loss = torch.nn.L1Loss()\n",
    "    return lambda_weight*reconstr_loss(real_im, reconstructed_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "lr_g=0.0002\n",
    "lr_d=0.0001\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "\n",
    "# g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
    "\n",
    "# Create optimizers for the generators and discriminators\n",
    "optimizer_G = torch.optim.Adam(filter(lambda p: p.requires_grad, itertools.chain(G_AtoB.parameters(), \n",
    "                                                                                 G_BtoA.parameters())),\n",
    "                               lr=lr_g, \n",
    "                               betas=(beta1, beta2))\n",
    "optimizer_D = torch.optim.Adam(filter(lambda p: p.requires_grad, itertools.chain(D_A.parameters(), \n",
    "                                                                                 D_B.parameters())), \n",
    "                               lr=lr_d, \n",
    "                               betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x):\n",
    "    \"\"\"Split data into image and mask (only assume 3-channel image)\"\"\"\n",
    "    return x[:, :3, :, :], x[:, 3:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader, test_dataloader, n_epochs=1000):\n",
    "    \n",
    "    print_every=10\n",
    "    losses = []\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        \n",
    "        for object_A, object_B in dataloader:\n",
    "\n",
    "            # =========================================\n",
    "            #            TRAIN THE GENERATORS\n",
    "            # =========================================\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            object_A = object_A.to(device)\n",
    "            object_B = object_B.to(device)\n",
    "            \n",
    "            fake_B = G_AtoB(object_A)\n",
    "            rec_A = G_BtoA(fake_B)\n",
    "            \n",
    "            image_b_fake, mask_b_fake = split(fake_B)\n",
    "            image_a_rec, mask_a_rec = split(rec_A)\n",
    "            \n",
    "            fake_A = G_BtoA(object_B)\n",
    "            rec_B = G_AtoB(fake_A)\n",
    "            \n",
    "            image_a_fake, mask_a_fake = split(fake_A)\n",
    "            image_b_rec, mask_b_rec = split(rec_B)\n",
    "            \n",
    "            set_requires_grad([D_A, D_B], False)\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            out_a = D_B(fake_B)\n",
    "            loss_G_A = real_mse_loss(out_a)\n",
    "            loss_cyc_A = cycle_consistency_loss(object_A, rec_A)\n",
    "            loss_idt_B = cycle_consistency_loss(G_BtoA(object_A), object_A)\n",
    "            weight_A = get_weight_for_ctx(object_A[:,3,:,:], mask_b_fake)\n",
    "            loss_ctx_A = self.weighted_L1_loss(object_A[:, :3,:,:], image_b_fake, weight=weight_A) * 10 * 10\n",
    "            \n",
    "            out_b = D_A(fake_A)\n",
    "            loss_G_B = real_mse_loss(out_a)\n",
    "            loss_cyc_B = cycle_consistency_loss(object_B, rec_B)\n",
    "            loss_idt_A = cycle_consistency_loss(G_AtoB(object_B), object_B)\n",
    "            weight_B = get_weight_for_ctx(object_B[:,3,:,:], mask_a_fake)\n",
    "            loss_ctx_B = self.weighted_L1_loss(object_A[:, :3,:,:], image_b_fake, weight=weight_B) * 10 * 10\n",
    "            \n",
    "            g_total_loss = loss_G_A + loss_cyc_A + loss_idt_B + loss_ctx_A + loss_G_B + loss_cyc_B + loss_idt_A + loss_ctx_B\n",
    "            g_total_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # ============================================\n",
    "            #            TRAIN THE DISCRIMINATORS\n",
    "            # ============================================\n",
    "            \n",
    "            set_requires_grad([D_A, D_B], True)\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            pred_real_B = D_B(object_B)\n",
    "            loss_D_real_B = real_mse_loss(object_B)\n",
    "            pred_fake_B = D_B(fake_B)\n",
    "            loss_D_fake_B = fake_mse_loss(pred_fake_B)\n",
    "            loss_D_B = (loss_D_real_B + loss_D_fake_B) / 2\n",
    "            loss_D_B.backward()\n",
    "            \n",
    "            pred_real_A = D_A(object_A)\n",
    "            loss_D_real_A = real_mse_loss(object_A)\n",
    "            pred_fake_A = D_A(fake_A)\n",
    "            loss_D_fake_A = fake_mse_loss(pred_fake_A)\n",
    "            loss_D_A = (loss_D_real_A + loss_D_fake_A) / 2\n",
    "            loss_D_A.backward()\n",
    "            \n",
    "            optimizer_D.step()\n",
    "            \n",
    "            if epoch % print_every == 0:\n",
    "                # append real and fake discriminator losses and the generator loss\n",
    "                losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "                print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\n",
    "                        epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "\n",
    "\n",
    "            sample_every=100\n",
    "            # Save the generated samples\n",
    "            if epoch % sample_every == 0:\n",
    "                G_YtoX.eval() # set generators to eval mode for sample generation\n",
    "                G_XtoY.eval()\n",
    "                save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16)\n",
    "                G_YtoX.train()\n",
    "                G_XtoY.train()\n",
    "\n",
    "        # uncomment these lines, if you want to save your model\n",
    "#         checkpoint_every=1000\n",
    "#         # Save the model parameters\n",
    "#         if epoch % checkpoint_every == 0:\n",
    "#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "training_loop(loader_pants, loader_pants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
